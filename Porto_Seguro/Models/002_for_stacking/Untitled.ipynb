{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def FeatureEngineering(trainpath,testpath):\n",
    "    #### Load Data\n",
    "    train = pd.read_csv(trainpath,nrows=10000)\n",
    "    test = pd.read_csv(testpath,nrows=10000)\n",
    "\n",
    "    ### \n",
    "    y = train['target'].values\n",
    "    testid= test['id'].values\n",
    "\n",
    "    train.drop(['id','target'],axis=1,inplace=True)\n",
    "    test.drop(['id'],axis=1,inplace=True)\n",
    "\n",
    "    ### Drop calc\n",
    "    unwanted = train.columns[train.columns.str.startswith('ps_calc_')]\n",
    "    train = train.drop(unwanted, axis=1)  \n",
    "    test = test.drop(unwanted, axis=1)\n",
    "\n",
    "    ### Great Recovery from Pascal's materpiece\n",
    "\n",
    "    def recon(reg):\n",
    "        integer = int(np.round((40*reg)**2)) \n",
    "        for a in range(32):\n",
    "            if (integer - a) % 31 == 0:\n",
    "                A = a\n",
    "        M = (integer - A)//31\n",
    "        return A, M\n",
    "    train['ps_reg_A'] = train['ps_reg_03'].apply(lambda x: recon(x)[0])\n",
    "    train['ps_reg_M'] = train['ps_reg_03'].apply(lambda x: recon(x)[1])\n",
    "    train['ps_reg_A'].replace(19,-1, inplace=True)\n",
    "    train['ps_reg_M'].replace(51,-1, inplace=True)\n",
    "    test['ps_reg_A'] = test['ps_reg_03'].apply(lambda x: recon(x)[0])\n",
    "    test['ps_reg_M'] = test['ps_reg_03'].apply(lambda x: recon(x)[1])\n",
    "    test['ps_reg_A'].replace(19,-1, inplace=True)\n",
    "    test['ps_reg_M'].replace(51,-1, inplace=True)\n",
    "    \n",
    "    trainX = train\n",
    "    testX = test\n",
    "    trainy = y\n",
    "    \n",
    "    return trainX, trainy, testX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gini(actual, pred, cmpcol = 0, sortcol = 1):\n",
    "    assert( len(actual) == len(pred) )\n",
    "    all = np.asarray(np.c_[ actual, pred, np.arange(len(actual)) ], dtype=np.float)\n",
    "    all = all[ np.lexsort((all[:,2], -1*all[:,1])) ]\n",
    "    totalLosses = all[:,0].sum()\n",
    "    giniSum = all[:,0].cumsum().sum() / totalLosses\n",
    " \n",
    "    giniSum -= (len(actual) + 1) / 2.\n",
    "    return giniSum / len(actual)\n",
    " \n",
    "def gini_normalized(a, p):\n",
    "    return gini(a, p) / gini(a, a)\n",
    "\n",
    "def gini_coefficient(preds,dtrain):\n",
    "    y = dtrain.get_label()\n",
    "    return 'gini', -gini_normalized(y,preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trainpath = \"/Users/guoli/Desktop/kaggle/Porto/train.csv\"\n",
    "testpath = \"/Users/guoli/Desktop/kaggle/Porto/test.csv\"\n",
    "\n",
    "trainX, trainy, testX = FeatureEngineering(trainpath,testpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Invalid parameter eval_metric for estimator XGBClassifier. Check the list of available parameters with `estimator.get_params().keys()`.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-2a21e4151e3b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0mxgb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mXGBClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0mxgb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0mx1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.25\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m99\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/guoli/anaconda2/lib/python2.7/site-packages/sklearn/base.pyc\u001b[0m in \u001b[0;36mset_params\u001b[0;34m(self, **params)\u001b[0m\n\u001b[1;32m    289\u001b[0m                                      \u001b[0;34m'Check the list of available parameters '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    290\u001b[0m                                      \u001b[0;34m'with `estimator.get_params().keys()`.'\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 291\u001b[0;31m                                      (key, self.__class__.__name__))\n\u001b[0m\u001b[1;32m    292\u001b[0m                 \u001b[0msetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    293\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Invalid parameter eval_metric for estimator XGBClassifier. Check the list of available parameters with `estimator.get_params().keys()`."
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "params = {'eta': 0.025, \n",
    "          'max_depth': 4, \n",
    "          'subsample': 0.9, \n",
    "          'colsample_bytree': 0.7, \n",
    "          'colsample_bylevel':0.7,\n",
    "          'min_child_weight':100,\n",
    "          'alpha':4,\n",
    "          'objective': 'binary:logistic', \n",
    "          'eval_metric': 'auc', \n",
    "          'seed': 99, \n",
    "          'silent': True}\n",
    "\n",
    "xgb = XGBClassifier()\n",
    "xgb.set_params(**params)\n",
    "x1, x2, y1, y2 = train_test_split(trainX, trainy, test_size=0.25, random_state=99)\n",
    "\n",
    "xgb.fit(x1, y1, \n",
    "        eval_set=[(x1,y1),(x2,y2)], \n",
    "        eval_metric=gini_coefficient,\n",
    "        early_stopping_rounds=100,\n",
    "        verbose=10)\n",
    "\n",
    "# x1, x2, y1, y2 = train_test_split(train, y, test_size=0.25, random_state=99)\n",
    "# watchlist = [(xgb.DMatrix(x1, y1), 'train'), (xgb.DMatrix(x2, y2), 'valid')]\n",
    "\n",
    "# model = xgb.train(params, xgb.DMatrix(x1, y1), 5000,  watchlist, feval=gini_xgb, maximize=True, \n",
    "#                   verbose_eval=100, early_stopping_rounds=70)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Clf4Stack(object):\n",
    "    def __init__(self, model, n_splits=5):\n",
    "        self.n_splits = n_splits\n",
    "        self.model = model\n",
    "\n",
    "    def fit_predict(self, trainX, trainy, testX):\n",
    "\n",
    "        self.train4stack = np.zeros(len(trainX))\n",
    "        self.test4stack = np.zeros(len(testX))\n",
    "\n",
    "        skf = StratifiedKFold(n_splits=self.n_splits, shuffle=True, random_state=44)\n",
    "\n",
    "        for train_index, test_index in skf.split(trainX, trainy):\n",
    "            X_train, X_test = trainX[train_index], trainX[test_index]\n",
    "            y_train, y_test = trainy[train_index], trainy[test_index]\n",
    "\n",
    "            self.model.fit(X_train, y_train)\n",
    "            y_pred = self.model.predict_proba(X_test)[:,1]\n",
    "            self.train4stack[test_index] = y_pred\n",
    "            self.test4stack += self.model.predict_proba(testX)[:,1]\n",
    "        \n",
    "        self.test4stack /= self.n_splits\n",
    "            \n",
    "    def output(self,train_file_name='train4stack.csv',\n",
    "                    test_file_name='test4stack.csv',\n",
    "                    col_name='F4stack'):\n",
    "\n",
    "        pd.DataFrame({col_name:self.train4stack}).to_csv(train_file_name,index=False) \n",
    "        pd.DataFrame({col_name:self.test4stack}).to_csv(test_file_name,index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
